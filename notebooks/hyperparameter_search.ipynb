{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.typing import NDArray"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from deep_learner import Tensor\n",
        "from deep_learner.datasets import cifar10\n",
        "from deep_learner.metrics.accuracy import accuracy\n",
        "from deep_learner.nn import (\n",
        "    CrossEntropyLoss,\n",
        "    Dropout,\n",
        "    Linear,\n",
        "    Module,\n",
        "    ReLU,\n",
        "    Sequential,\n",
        "    Softmax,\n",
        ")\n",
        "from deep_learner.nn.optimizer.sgd import SGD\n",
        "from deep_learner.utils import batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_X, train_Y, test_X, test_Y = cifar10()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def describe(data: NDArray) -> None:\n",
        "    print(\"Shape:\", data.shape)\n",
        "    print(f\"Min: {data.min()}, Max: {data.max()}\")\n",
        "    print(f\"Mean: {data.mean()}, Standard deviation: {data.std()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape: (50000, 3072)\n",
            "Min: 0.0, Max: 1.0\n",
            "Mean: 0.4733648896217346, Standard deviation: 0.25156906247138977\n",
            "Shape: (10000, 3072)\n",
            "Min: 0.0, Max: 1.0\n",
            "Mean: 0.4765852391719818, Standard deviation: 0.25121963024139404\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "describe(train_X), describe(test_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocessing(train_data: NDArray, test_data: NDArray) -> tuple[NDArray, NDArray]:\n",
        "    return (train_data - train_data.mean()) / train_data.std(), (\n",
        "        test_data - test_data.mean()\n",
        "    ) / test_data.std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape: (50000, 3072)\n",
            "Min: -1.8816498517990112, Max: 2.0934016704559326\n",
            "Mean: -7.475010534108151e-06, Standard deviation: 1.0\n",
            "Shape: (10000, 3072)\n",
            "Min: -1.8970860242843628, Max: 2.0834946632385254\n",
            "Mean: -1.214090957546432e-06, Standard deviation: 1.0000001192092896\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "p_train_X, p_test_X = preprocessing(train_X, test_X)\n",
        "\n",
        "describe(p_train_X), describe(p_test_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((50000, 10),\n",
              " array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
              "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
              "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]], dtype=uint8))"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_Y.shape, train_Y[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MyModel(Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.linear1 = Linear(n_in=3072, n_out=256)\n",
        "        self.linear2 = Linear(n_in=256, n_out=256)\n",
        "        self.linear3 = Linear(n_in=256, n_out=10)\n",
        "\n",
        "        # self.skip_connection = Linear(n_in=256, n_out=256)\n",
        "\n",
        "        self.relu = ReLU()\n",
        "\n",
        "        self.softmax = Softmax()\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self.softmax(\n",
        "            self.linear3(self.relu(self.linear2(self.relu(self.linear1(x)))))\n",
        "        )\n",
        "\n",
        "\n",
        "model = Sequential(\n",
        "    Linear(n_in=3072, n_out=256),\n",
        "    ReLU(),\n",
        "    Linear(n_in=256, n_out=256),\n",
        "    ReLU(),\n",
        "    Dropout(drop_proba=0.4),\n",
        "    Linear(n_in=256, n_out=10),\n",
        "    Softmax(),\n",
        ")\n",
        "\n",
        "loss_fn = CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections.abc import Generator, Sequence\n",
        "from itertools import product\n",
        "from typing import Any\n",
        "\n",
        "\n",
        "def grid_search(\n",
        "    hyperparameters: dict[str, Sequence[Any]],\n",
        ") -> Generator[dict, None, None]:\n",
        "    for combination in product(*hyperparameters.values()):\n",
        "        yield {\n",
        "            key: param for key, param in zip(hyperparameters, combination, strict=False)\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'lr': 0.1, 'batch_size': 16}\n",
            "{'lr': 0.1, 'batch_size': 32}\n",
            "{'lr': 0.1, 'batch_size': 128}\n",
            "{'lr': 0.01, 'batch_size': 16}\n",
            "{'lr': 0.01, 'batch_size': 32}\n",
            "{'lr': 0.01, 'batch_size': 128}\n"
          ]
        }
      ],
      "source": [
        "for param in grid_search({\"lr\": [1e-1, 1e-2], \"batch_size\": [16, 32, 128]}):\n",
        "    print(param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch=0, train_accuracy=23.90%, cum_loss=417.8727\n",
            "epoch=1, train_accuracy=32.16%, cum_loss=375.4670\n",
            "epoch=2, train_accuracy=35.69%, cum_loss=355.7024\n",
            "epoch=3, train_accuracy=38.03%, cum_loss=343.3465\n",
            "epoch=4, train_accuracy=39.88%, cum_loss=334.1354\n",
            "epoch=5, train_accuracy=41.28%, cum_loss=326.9482\n",
            "epoch=6, train_accuracy=42.22%, cum_loss=320.6745\n",
            "epoch=7, train_accuracy=43.57%, cum_loss=315.4324\n",
            "epoch=8, train_accuracy=44.39%, cum_loss=310.1797\n",
            "epoch=9, train_accuracy=45.04%, cum_loss=305.9832\n",
            "epoch=10, train_accuracy=45.96%, cum_loss=302.1980\n",
            "epoch=11, train_accuracy=46.62%, cum_loss=297.8916\n",
            "epoch=12, train_accuracy=47.32%, cum_loss=294.3786\n",
            "epoch=13, train_accuracy=47.97%, cum_loss=290.5426\n",
            "epoch=14, train_accuracy=48.51%, cum_loss=287.4820\n",
            "epoch=15, train_accuracy=49.35%, cum_loss=283.8905\n",
            "epoch=16, train_accuracy=49.79%, cum_loss=281.0463\n",
            "epoch=17, train_accuracy=50.33%, cum_loss=278.0836\n",
            "epoch=18, train_accuracy=51.16%, cum_loss=275.1289\n",
            "epoch=19, train_accuracy=51.49%, cum_loss=272.1997\n",
            "epoch=20, train_accuracy=51.95%, cum_loss=269.6328\n",
            "epoch=21, train_accuracy=52.48%, cum_loss=266.4876\n",
            "epoch=22, train_accuracy=52.83%, cum_loss=264.2002\n",
            "epoch=23, train_accuracy=53.42%, cum_loss=262.0319\n",
            "epoch=24, train_accuracy=53.76%, cum_loss=259.5983\n",
            "epoch=25, train_accuracy=53.99%, cum_loss=257.2952\n",
            "epoch=26, train_accuracy=54.96%, cum_loss=254.4703\n",
            "epoch=27, train_accuracy=55.27%, cum_loss=252.2626\n",
            "epoch=28, train_accuracy=55.65%, cum_loss=250.0501\n",
            "epoch=29, train_accuracy=55.84%, cum_loss=247.6926\n",
            "epoch=30, train_accuracy=56.54%, cum_loss=244.7517\n",
            "epoch=31, train_accuracy=56.89%, cum_loss=242.2633\n",
            "epoch=32, train_accuracy=57.16%, cum_loss=240.2924\n",
            "epoch=33, train_accuracy=57.79%, cum_loss=238.1968\n",
            "epoch=34, train_accuracy=57.98%, cum_loss=236.5130\n",
            "epoch=35, train_accuracy=58.56%, cum_loss=234.3663\n",
            "epoch=36, train_accuracy=58.93%, cum_loss=232.2894\n",
            "epoch=37, train_accuracy=59.32%, cum_loss=229.6449\n",
            "epoch=38, train_accuracy=59.41%, cum_loss=228.5438\n",
            "epoch=39, train_accuracy=60.02%, cum_loss=225.4515\n",
            "epoch=40, train_accuracy=60.33%, cum_loss=223.4307\n",
            "epoch=41, train_accuracy=60.69%, cum_loss=221.4845\n",
            "epoch=42, train_accuracy=61.13%, cum_loss=219.4668\n",
            "epoch=43, train_accuracy=61.50%, cum_loss=217.4331\n",
            "epoch=44, train_accuracy=61.73%, cum_loss=215.5524\n",
            "epoch=45, train_accuracy=62.09%, cum_loss=213.2815\n",
            "epoch=46, train_accuracy=62.29%, cum_loss=211.6383\n",
            "epoch=47, train_accuracy=62.88%, cum_loss=209.2945\n",
            "epoch=48, train_accuracy=63.09%, cum_loss=207.9539\n",
            "epoch=49, train_accuracy=63.57%, cum_loss=205.6556\n",
            "epoch=50, train_accuracy=63.85%, cum_loss=203.6341\n",
            "epoch=51, train_accuracy=64.20%, cum_loss=202.1511\n",
            "epoch=52, train_accuracy=64.37%, cum_loss=200.0825\n",
            "epoch=53, train_accuracy=64.74%, cum_loss=198.0328\n",
            "epoch=54, train_accuracy=65.25%, cum_loss=196.1544\n",
            "epoch=55, train_accuracy=65.30%, cum_loss=194.5593\n",
            "epoch=56, train_accuracy=65.60%, cum_loss=192.9980\n",
            "epoch=57, train_accuracy=66.27%, cum_loss=190.9446\n",
            "epoch=58, train_accuracy=66.18%, cum_loss=189.2522\n",
            "epoch=59, train_accuracy=66.61%, cum_loss=187.5717\n",
            "epoch=60, train_accuracy=66.91%, cum_loss=186.0259\n",
            "epoch=61, train_accuracy=67.32%, cum_loss=183.6115\n",
            "epoch=62, train_accuracy=67.47%, cum_loss=182.8511\n",
            "epoch=63, train_accuracy=67.88%, cum_loss=180.7420\n",
            "epoch=64, train_accuracy=68.10%, cum_loss=178.4880\n",
            "epoch=65, train_accuracy=68.55%, cum_loss=177.3332\n",
            "epoch=66, train_accuracy=68.81%, cum_loss=174.9800\n",
            "epoch=67, train_accuracy=69.03%, cum_loss=173.6196\n",
            "epoch=68, train_accuracy=69.62%, cum_loss=171.2847\n",
            "epoch=69, train_accuracy=69.54%, cum_loss=171.1794\n",
            "epoch=70, train_accuracy=69.96%, cum_loss=168.6236\n",
            "epoch=71, train_accuracy=70.17%, cum_loss=168.1002\n",
            "epoch=72, train_accuracy=70.42%, cum_loss=166.5799\n",
            "epoch=73, train_accuracy=70.88%, cum_loss=164.4457\n",
            "epoch=74, train_accuracy=71.12%, cum_loss=163.3358\n",
            "epoch=75, train_accuracy=71.15%, cum_loss=161.1562\n",
            "epoch=76, train_accuracy=71.64%, cum_loss=160.6297\n",
            "epoch=77, train_accuracy=72.18%, cum_loss=156.8341\n",
            "epoch=78, train_accuracy=72.16%, cum_loss=157.0437\n",
            "epoch=79, train_accuracy=72.45%, cum_loss=155.1213\n",
            "epoch=80, train_accuracy=72.81%, cum_loss=153.4825\n",
            "epoch=81, train_accuracy=72.94%, cum_loss=152.3706\n",
            "epoch=82, train_accuracy=73.26%, cum_loss=150.5577\n",
            "epoch=83, train_accuracy=73.35%, cum_loss=150.1367\n",
            "epoch=84, train_accuracy=73.43%, cum_loss=149.9600\n",
            "epoch=85, train_accuracy=73.95%, cum_loss=147.5270\n",
            "epoch=86, train_accuracy=74.12%, cum_loss=145.9374\n",
            "epoch=87, train_accuracy=74.28%, cum_loss=144.9868\n",
            "epoch=88, train_accuracy=74.77%, cum_loss=143.3625\n",
            "epoch=89, train_accuracy=74.74%, cum_loss=142.4280\n",
            "epoch=90, train_accuracy=74.86%, cum_loss=141.6328\n",
            "epoch=91, train_accuracy=75.40%, cum_loss=138.8681\n",
            "epoch=92, train_accuracy=75.85%, cum_loss=136.6582\n",
            "epoch=93, train_accuracy=76.05%, cum_loss=135.0611\n",
            "epoch=94, train_accuracy=76.03%, cum_loss=137.0603\n",
            "epoch=95, train_accuracy=75.99%, cum_loss=135.8582\n",
            "epoch=96, train_accuracy=76.39%, cum_loss=132.3148\n",
            "epoch=97, train_accuracy=76.91%, cum_loss=130.5575\n",
            "epoch=98, train_accuracy=77.19%, cum_loss=128.9848\n",
            "epoch=99, train_accuracy=77.72%, cum_loss=127.3934\n"
          ]
        }
      ],
      "source": [
        "model.train()\n",
        "\n",
        "optimizer = SGD(model, learning_rate=1e-2)\n",
        "\n",
        "for epoch in range(100):\n",
        "    cum_loss = Tensor(0)\n",
        "    num_batches = 0\n",
        "    train_accuracy = Tensor(0)\n",
        "\n",
        "    for batch_X, batch_Y in batch(p_train_X, train_Y, batch_size=256):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        X = Tensor(batch_X)\n",
        "        Y = Tensor(batch_Y)\n",
        "\n",
        "        predictions: Tensor = model(X)\n",
        "        loss = loss_fn(predictions, Y)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        cum_loss += loss\n",
        "        num_batches += 1\n",
        "\n",
        "        train_accuracy += accuracy(\n",
        "            Tensor(np.argmax(predictions.data, axis=-1)),\n",
        "            Tensor(np.argmax(Y.data, axis=-1)),\n",
        "        )\n",
        "\n",
        "    print(\n",
        "        f\"{epoch=}, train_accuracy={train_accuracy.data / num_batches:.2%}, cum_loss={cum_loss.data:.4f}\"\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "deep-learner-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
