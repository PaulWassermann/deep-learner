{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Understanding the difficulty of training deep feedforward neural networks\n",
        "par Xavier Glorot et Yoshua Bengio (2010)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO\n",
        "\n",
        "- Ajouter un petit set sur lequel calculer les diff\u00e9rentes m\u00e9triques de monitoring (moyenne / std / histogramme des activations et gradients)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "from itertools import chain\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from numpy.typing import NDArray\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deep Learner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from deep_learner import Tensor\n",
        "from deep_learner._core.types import Device\n",
        "from deep_learner.datasets import cifar10, mnist\n",
        "from deep_learner.metrics.accuracy import accuracy\n",
        "from deep_learner.nn import (\n",
        "    SGD,\n",
        "    CrossEntropyLoss,\n",
        "    Linear,\n",
        "    Module,\n",
        "    Optimizer,\n",
        "    Sequential,\n",
        "    Sigmoid,\n",
        "    Softmax,\n",
        "    Softsign,\n",
        "    Tanh,\n",
        ")\n",
        "from deep_learner.utils import batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CIFAR10_DATASET = cifar10()\n",
        "MNIST_DATASET = mnist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_X, val_X, train_Y, val_Y = train_test_split(\n",
        "    CIFAR10_DATASET[0],\n",
        "    CIFAR10_DATASET[1],\n",
        "    test_size=10_000,\n",
        "    random_state=0,\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "test_X, monitoring_X, test_Y, monitoring_Y = train_test_split(\n",
        "    CIFAR10_DATASET[2], CIFAR10_DATASET[3], test_size=300, random_state=0, shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = {}\n",
        "\n",
        "for num_hidden in range(1, 6):\n",
        "    for activation_fn in (Sigmoid, Softsign, Tanh):\n",
        "        act_fn_str = activation_fn.__name__.lower()\n",
        "        models[f\"model_{act_fn_str}_{num_hidden}\"] = Sequential(\n",
        "            Linear(3072, 1_000),\n",
        "            activation_fn(),\n",
        "            *list(\n",
        "                chain.from_iterable(\n",
        "                    (Linear(1_000, 1_000), activation_fn())\n",
        "                    for _ in range(num_hidden - 1)\n",
        "                ),\n",
        "            ),\n",
        "            Linear(1_000, 10),\n",
        "            Softmax(),\n",
        "        )\n",
        "\n",
        "print(models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hooks to collect activations and gradients statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models_statistics = {model_name: {} for model_name in models}\n",
        "\n",
        "\n",
        "def get_activations(accumulator):\n",
        "    def hook(module, outputs, *inputs, **kwargs):\n",
        "        accumulator[\"act_mean\"].append(float(outputs.data.mean()))\n",
        "        accumulator[\"act_std\"].append(float(outputs.data.std()))\n",
        "\n",
        "    return hook\n",
        "\n",
        "\n",
        "def get_gradients(accumulator):\n",
        "    def hook(grad):\n",
        "        accumulator[\"grad_mean\"].append(float(grad.data.mean()))\n",
        "        accumulator[\"grad_std\"].append(float(grad.data.std()))\n",
        "\n",
        "    return hook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# In the paper, the procedure to tune the learning rate is not explicited, although it\n",
        "# appears to be some kind of search validated through a validation set,\n",
        "# therefore we simply set it to 10^-3 for the moment\n",
        "LEARNING_RATE: float = 1e-3\n",
        "BATCH_SIZE: int = 10\n",
        "EPOCHS: int = 10\n",
        "METRICS_PER_EPOCH: int = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(\n",
        "    model: Module,\n",
        "    epochs: int,\n",
        "    batch_size: int,\n",
        "    optimizer: Optimizer,\n",
        "    loss_fn: Module,\n",
        "    train_X: NDArray,\n",
        "    train_Y: NDArray,\n",
        "    monitoring_X: NDArray,\n",
        "    monitoring_Y: NDArray,\n",
        "    monitoring_acc: dict[str, dict[str, list[float]]],\n",
        "    device: Device,\n",
        ") -> Module:\n",
        "    model.to(device)\n",
        "\n",
        "    num_batches = len(train_X) // batch_size + bool(len(train_X) % batch_size)\n",
        "    monitoring_X = Tensor(monitoring_X).to(device)\n",
        "    monitoring_Y = Tensor(monitoring_Y).to(device)\n",
        "\n",
        "    for epoch in tqdm(range(epochs), desc=\"Epoch:\", total=epochs):\n",
        "        model.train()\n",
        "\n",
        "        batch_counter = 0\n",
        "        epoch_loss = Tensor(0)\n",
        "        epoch_accuracy = Tensor(0)\n",
        "\n",
        "        for batch_X, batch_Y in batch(train_X, train_Y, batch_size=batch_size):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            inputs = Tensor(batch_X).to(device)\n",
        "            labels = Tensor(batch_Y).to(device)\n",
        "\n",
        "            predictions = model(inputs)\n",
        "\n",
        "            loss = loss_fn(predictions, labels)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_accuracy += accuracy(\n",
        "                Tensor(np.argmax(predictions.detach().to(Device.CPU).data, axis=-1)),\n",
        "                Tensor(np.argmax(labels.detach().to(Device.CPU).data, axis=-1)),\n",
        "            )\n",
        "\n",
        "            batch_counter += 1\n",
        "            epoch_loss += loss.detach().to(Device.CPU)\n",
        "\n",
        "            if (batch_counter) % (num_batches // METRICS_PER_EPOCH) == 0:\n",
        "                handles = []\n",
        "                for child_name, child in model.named_children():\n",
        "                    if isinstance(child, Linear):\n",
        "                        if child_name not in monitoring_acc:\n",
        "                            monitoring_acc[child_name] = {\n",
        "                                \"act_mean\": [],\n",
        "                                \"act_std\": [],\n",
        "                                # \"grad_mean\": [],\n",
        "                                # \"grad_std\": []\n",
        "                            }\n",
        "                        handles.append(\n",
        "                            child.register_forward_hook(\n",
        "                                get_activations(monitoring_acc[child_name])\n",
        "                            )\n",
        "                        )\n",
        "                        # handles.append(child.register_backward_hook(get_gradients(monitoring_acc[child_name])))\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                monitoring_preds = model(monitoring_X)\n",
        "                monitoring_loss = loss_fn(monitoring_preds, monitoring_Y)\n",
        "\n",
        "                monitoring_loss.backward()\n",
        "\n",
        "                for handle in handles:\n",
        "                    handle.remove()\n",
        "\n",
        "        print(\n",
        "            f\"[+] [Epoch {epoch + 1}] mean accuracy: {epoch_accuracy.data / batch_counter:.2%}, mean loss: {epoch_loss.data / batch_counter:.6f}\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "i = 0\n",
        "device = Device.CPU\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"[+] Training {model_name}...\")\n",
        "    train(\n",
        "        model,\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        optimizer=SGD(model, LEARNING_RATE),\n",
        "        loss_fn=CrossEntropyLoss(),\n",
        "        train_X=train_X,\n",
        "        train_Y=train_Y,\n",
        "        monitoring_X=monitoring_X,\n",
        "        monitoring_Y=monitoring_Y,\n",
        "        monitoring_acc=models_statistics[model_name],\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    test_predictions = model(Tensor(test_X).to(device))\n",
        "    test_accuracy = accuracy(\n",
        "        Tensor(np.argmax(test_predictions.detach().to(Device.CPU).data, axis=-1)),\n",
        "        Tensor(np.argmax(test_Y, axis=-1)),\n",
        "    )\n",
        "\n",
        "    print(f\"[+] Accuracy on test set: {test_accuracy.data:.2%}\\n\")\n",
        "\n",
        "    i += 1\n",
        "    if i == 3:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "colors = [\"r\", \"g\", \"b\", \"c\", \"b\"]\n",
        "markers = [\"o\", \"^\", \"s\", \"p\", \"h\"]\n",
        "\n",
        "x_axis = range(\n",
        "    0,\n",
        "    EPOCHS * len(train_X) // BATCH_SIZE,\n",
        "    len(train_X) // (BATCH_SIZE * METRICS_PER_EPOCH),\n",
        ")\n",
        "\n",
        "for model_name, layers_statistics in models_statistics.items():\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
        "\n",
        "    for index, (layer_name, layer_statistics) in enumerate(layers_statistics.items()):\n",
        "        axs[0].errorbar(\n",
        "            x_axis,\n",
        "            layer_statistics[\"act_mean\"],\n",
        "            yerr=layer_statistics[\"act_std\"],\n",
        "            errorevery=(index, len(layers_statistics)),\n",
        "            color=colors[index],\n",
        "            label=layer_name,\n",
        "        )\n",
        "\n",
        "    axs[0].legend()\n",
        "    axs[0].set_xlabel(\"Number of updates\")\n",
        "    axs[0].set_ylabel(\"Layer activations mean values\")\n",
        "    axs[0].set_title(\"Activations\")\n",
        "\n",
        "    axs[1].legend()\n",
        "    axs[1].set_xlabel(\"Number of updates\")\n",
        "    axs[1].set_ylabel(\"Layer gradients mean values\")\n",
        "    axs[1].set_title(\"Gradients\")\n",
        "\n",
        "    fig.suptitle(f\"Statistics for {model_name}\")\n",
        "    fig.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "deep-learner-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
