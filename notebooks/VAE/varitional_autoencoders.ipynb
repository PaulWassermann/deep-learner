{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fb38b6c",
   "metadata": {},
   "source": [
    "# Variational Autoencoders (VAEs)\n",
    "\n",
    "A VAE is a generative model. Given some samples $x$ drawn from a distribution $D$ (our data), we want to approximate the true distribution $p^*(x)$ with a parametrized distribution $p_\\theta(x)$:\n",
    "\n",
    "given $x \\sim D$, we want to optimize for $\\theta$ so that \n",
    "\n",
    "\\begin{equation*}\n",
    "    p_\\theta(x) \\approx p^*(x)\n",
    "\\end{equation*}\n",
    "\n",
    "$p_\\theta(x)$ is called the *likelihood*, thus we are in presence of a *Maximum Likelihood Estimation* (MLE) problem.\n",
    "\n",
    "We assume that each sample of our dataset can be described by a finite set of characteristics called the *latent variables*, that we call $z$. These characteristics are called latent as they are underlying: they capture the variations in our data samples but are not directly observed. To build a better intuition on what latent variables are, consider the following example drawn from [[2]](#sources):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91e8c1e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_examples = 3\n",
    "\n",
    "fig, axs = plt.subplots(1, num_examples)\n",
    "\n",
    "for ax_index in range(num_examples):\n",
    "    radius = randint(1, 10) / 20\n",
    "    position = (randint(0, 10) / 10, randint(0, 10) / 10)\n",
    "\n",
    "    ax = axs[ax_index]\n",
    "    ax.set_aspect(\"equal\", \"box\")\n",
    "    ax.add_artist(mpatches.Circle(position, radius))\n",
    "    ax.add_artist(mpatches.Rectangle((-0.6, -0.6), width=2.2, height=2.2, fill=False))\n",
    "\n",
    "    ax.set_xlim((-0.7, 1.7))\n",
    "    ax.set_ylim((-0.7, 1.7))\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    ax.set_title(f\"r={radius}, p={position}\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74927916",
   "metadata": {},
   "source": [
    "In the example above, our dataset is composed of images of blue circles ; the underlying characteristics of any given circle are its radius and its position. Those two latent variables are not directly observed on our images, although they can be inferred. On a more complex case like handwritten digits, latent variables could be the size of the stroke, the position, the stroke style, ...\n",
    "\n",
    "It now appears that each and every sample can be described in the *data space* or the *latent space*. A VAE is able to perform both inference and generation:\n",
    "\n",
    "* **inference**: infer the latent variables $z$ from the data point $x$ i.e computing $p_\\theta(z|x)$\n",
    "* **generation**: generate a data point $x$ from a the latent variables $z$ i.e computing $p_\\theta(x|z)$\n",
    "\n",
    "Remembering we want to adjust our set of parameters $\\theta$ such that we maximise our likelihood $p_\\theta(x)$ for any data point $x$, we can write the marginal likelihood as:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        p_\\theta(x) & = \\int_z p_\\theta(x, z) dz \\\\\n",
    "        & = \\int_z p_\\theta(z) p_\\theta(x|z) dz\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "It turns out this integral is intractable: there are no analytical solutions and its numerical computation require an amount of samples that grow exponentially with the number of latent variables.\n",
    "\n",
    "<span id=\"eq1\"></span>\n",
    "\n",
    "However, Bayes can bail us out of this seemingly impossible situation:\n",
    "\n",
    "\\begin{align*}\n",
    "    p_\\theta(x) & = \\frac{p_\\theta(x, z)}{p_\\theta(z|x)} \\\\\n",
    "    & = \\frac{p_\\theta(z) p_\\theta(x|z)}{p_\\theta(z|x)}\n",
    "\\end{align*}\n",
    "\n",
    "where:\n",
    "\n",
    "* $p_\\theta(x|z)$ is the likelihood\n",
    "* $p_\\theta(x)$ is the marginal likelihood (marginalized over all possible $z$)\n",
    "* $p_\\theta(z)$ is the *prior* distribution (it can be understood as the probability of our model before having seen any data)\n",
    "* $p_\\theta(z|x)$ is the *posterior* distribution\n",
    "\n",
    "![VAE schema](VAE.png \"VAE schema\")\n",
    "\n",
    "We already concluded that the computation of $p_\\theta(x)$ is out of our reach. However, in order to maximize our marginal likelihood, we see from <a href=\"#eq1\">(1)</a> that knowing the posterior distribution could be enough to reach our goal. Since we don't know the analytical form of the posterior, we can approximate it with a known distribution $q_\\phi(z|x)$. This is the core principle of variationnal inference [[3]](#sources), optimizing $\\phi$ for \n",
    "\n",
    "\\begin{equation*}\n",
    "    q_\\phi(z|x) \\approx p_\\theta(z|x)\n",
    "\\end{equation*}\n",
    "\n",
    "Two things are left for us to do:\n",
    "\n",
    "1) Choosing the \"form\" of the distribution (gaussian, Poisson, ...)\n",
    "2) Optimizing our chosen distribution with respect to its parameters $\\phi$\n",
    "\n",
    "In order to measure how far our approximate distribution lies from the actual posterior distribution, we rely on the Kullback-Leibler divergence [[4]](#sources).\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    NOTE: KL-divergence is not a distance as it is not symmetrical !\n",
    "</div>\n",
    "\n",
    "Unwrapping the raw Kullback-Leibler equation, we can write:\n",
    "\n",
    "\\begin{align*}\n",
    "    D_{KL}(q_\\phi || p_\\theta) & = \\int_z q_\\phi(z|x) \\log \\left( \\frac{q_\\phi(z|x)}{p_\\theta(z|x)} \\right) dz \\\\\n",
    "    & = \\int_z q_\\phi(z|x) \\log \\left( \\frac{q_\\phi(z|x) p_\\theta(x)}{p_\\theta(x, z)} \\right) dz \\\\\n",
    "    & = \\int_z q_\\phi(z|x) \\log(q_\\phi(z|x)) dz + \\int_z q_\\phi(z|x) \\log(p_\\theta(x)) dz - \\int_z q_\\phi(z|x) \\log(p_\\theta(x, z)) dz \\\\\n",
    "    & = \\mathbb{E}_{z \\sim q_\\phi(\\cdot|x)} \\left[ \\log(q_\\phi(z|x)) \\right] + \\log(p_\\theta(x)) - \\mathbb{E}_{z \\sim q_\\phi(\\cdot|x)} \\left[ \\log(p_\\theta(x, z)) \\right] \\\\\n",
    "    & = \\log(p_\\theta(x)) - \\mathbb{E}_{z \\sim q_\\phi(\\cdot|x)} \\left[ \\log(p_\\theta(x, z)) - \\log(q_\\phi(z|x)) \\right]\n",
    "\\end{align*}\n",
    "\n",
    "We derived a new expression for our marginal log-likelihood :\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\log(p_\\theta(x)) = \\mathbb{E}_{z \\sim q_\\phi(\\cdot|x)} \\left[ \\log \\left( \\frac{p_\\theta(x, z)}{q_\\phi(z|x)} \\right) \\right] + D_{KL}(q_\\phi || p_\\theta)\n",
    "\\end{equation*}\n",
    "\n",
    "Maximizing the log-likelihood is the same as maximizing the expected value of $\\log \\left( \\frac{p_\\theta(x, z)}{q_\\phi(z|x)} \\right)$ because the KL-divergence term is greater than zero ([proof](#appendix-a---proving-the-positivity-of-the-kullback-leibler-divergence)). This term is called the Evidence Lower BOund, or ELBO for short, because:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbb{E}_{z \\sim q_\\phi(\\cdot|x)} \\left[ \\log \\left( \\frac{p_\\theta(x, z)}{q_\\phi(z|x)} \\right) \\right] \\le \\log(p_\\theta(x))\n",
    "\\end{equation*}\n",
    "\n",
    "Finally, writing the equation as:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathbb{E}_{z \\sim q_\\phi(\\cdot|x)} \\left[ \\log \\left( \\frac{p_\\theta(x, z)}{q_\\phi(z|x)} \\right) \\right] = \\log(p_\\theta(x)) - D_{KL}(q_\\phi || p_\\theta)\n",
    "\\end{equation*}\n",
    "\n",
    "we see hat maximizing the expected value will both maximize the likelihood and minimize the KL divergence. However, we don't know the joint probability distribution of the observed data and the latent variables. We still need to work a bit more :\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbb{E}_{z \\sim q_\\phi(\\cdot | x)} \\left[ \\log \\left( \\frac{p_\\theta(x, z)}{q_\\phi(z|x)} \\right) \\right] & = \\mathbb{E}_{z \\sim q_\\phi(\\cdot | x)} \\left[ \\log \\left( \\frac{p_\\theta(x|z) p_\\theta(z)}{q_\\phi(z|x)} \\right) \\right] \\tag{Bayes' theorem} \\\\\n",
    "    & = \\mathbb{E}_{z \\sim q_\\phi(\\cdot | x)} \\left[ \\log(p_\\theta(x|z)) + \\log \\left( \\frac{p_\\theta(z)}{q_\\phi(z|x)} \\right) \\right] \\\\\n",
    "    & = \\mathbb{E}_{z \\sim q_\\phi(\\cdot | x)} \\left[ \\log(p_\\theta(x|z)) \\right] + \\mathbb{E}_{z \\sim q_\\phi(\\cdot | x)} \\left[ \\log \\left( \\frac{p_\\theta(z)}{q_\\phi(z|x)} \\right)\\right] \\tag{expectation linearity} \\\\\n",
    "    & = \\log(p_\\theta(x|z)) - D_{KL} (q_\\phi(z|x) || p_\\theta(z)) \\\\\n",
    "    & \\le \\log(p_\\theta(x))\n",
    "\\end{align*}\n",
    "\n",
    "Instead of a maximization problem, we prefer to solve our loss function. Therefore, we define our training loss as such :\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\mathcal{L}_{\\theta, \\phi}(x) = D_{KL} (q_\\phi(z|x) || p_\\theta(z)) - \\log(p_\\theta(x|z))\n",
    "\\end{equation*}\n",
    "\n",
    "where :\n",
    "\n",
    "* $- \\log(p_\\theta(x|z))$ is the reconstruction term; the lower it gets, the closer the decoded output is to the input\n",
    "* $D_{KL} (q_\\phi(z|x) || p_\\theta(z))$ is a regularization term on the latent space; it solely depends on our choices of $p_\\theta(z)$ and $q_\\phi(z|x)$\n",
    "\n",
    "Speaking of choosing our prior and our posterior distributions, we make the following assumptions :\n",
    "\n",
    "* $p_\\theta(z) \\sim \\mathcal{N}(0, I_d)$\n",
    "* $q_\\phi(z|x) \\sim \\mathcal{N}(\\vec{\\mu_d}(\\phi, x), diag(\\vec{\\sigma_d}(\\phi, x)^2))$\n",
    "\n",
    "where $d$ is the dimension of the latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855d3d19",
   "metadata": {},
   "source": [
    "## Implementing the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11513d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "    encoder: Sequential(\n",
       "        (0): Linear(n_in=784, n_out=1568),\n",
       "        (1): ReLU(),\n",
       "        (2): Linear(n_in=1568, n_out=392),\n",
       "        (3): ReLU(),\n",
       "        (4): Linear(n_in=392, n_out=98),\n",
       "        (5): ReLU(),\n",
       "        (6): Linear(n_in=98, n_out=8),\n",
       "        (7): ReLU()\n",
       "    ),\n",
       "    mean_layer: Linear(n_in=8, n_out=8),\n",
       "    log_variance_layer: Linear(n_in=8, n_out=8),\n",
       "    decoder: Sequential(\n",
       "        (0): Linear(n_in=8, n_out=98),\n",
       "        (1): ReLU(),\n",
       "        (2): Linear(n_in=98, n_out=392),\n",
       "        (3): ReLU(),\n",
       "        (4): Linear(n_in=392, n_out=1568),\n",
       "        (5): ReLU(),\n",
       "        (6): Linear(n_in=1568, n_out=784)\n",
       "    )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "from deep_learner import Tensor, exponential, mean, ones, rand_normal, zeros\n",
    "from deep_learner.nn import Linear, Module, ReLU, Sequential\n",
    "from deep_learner.functional import mean_squared_error\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VAEEncoderOutput:\n",
    "    mean: Tensor\n",
    "    log_variance: Tensor\n",
    "    latent: Tensor\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VAEOutput:\n",
    "    encoder: VAEEncoderOutput\n",
    "    decoder: Tensor\n",
    "\n",
    "\n",
    "class VAE(Module):\n",
    "    def __init__(self, input_dimension: int, latent_dimension: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Sequential(\n",
    "            Linear(n_in=input_dimension, n_out=2 * input_dimension),\n",
    "            ReLU(),\n",
    "            Linear(n_in=2 * input_dimension, n_out=input_dimension // 2),\n",
    "            ReLU(),\n",
    "            Linear(n_in=input_dimension // 2, n_out=input_dimension // 8),\n",
    "            ReLU(),\n",
    "            Linear(n_in=input_dimension // 8, n_out=latent_dimension),\n",
    "            ReLU(),\n",
    "        )\n",
    "\n",
    "        self.mean_layer = Linear(n_in=latent_dimension, n_out=latent_dimension)\n",
    "        self.log_variance_layer = Linear(n_in=latent_dimension, n_out=latent_dimension)\n",
    "\n",
    "        self.decoder = Sequential(\n",
    "            Linear(n_in=latent_dimension, n_out=input_dimension // 8),\n",
    "            ReLU(),\n",
    "            Linear(n_in=input_dimension // 8, n_out=input_dimension // 2),\n",
    "            ReLU(),\n",
    "            Linear(n_in=input_dimension // 2, n_out=2 * input_dimension),\n",
    "            ReLU(),\n",
    "            Linear(n_in=2 * input_dimension, n_out=input_dimension),\n",
    "        )\n",
    "\n",
    "    def compute_loss(\n",
    "        self, x: Tensor, y: Tensor, mean_: Tensor, log_variance: Tensor\n",
    "    ) -> Tensor:\n",
    "        reconstruction_loss = mean_squared_error(x, y)\n",
    "        kl_divergence = mean(\n",
    "            exponential(log_variance) + mean_**2 - log_variance - Tensor(1).to(x.device)\n",
    "        ) / Tensor(2).to(x.device)\n",
    "\n",
    "        return kl_divergence + reconstruction_loss\n",
    "\n",
    "    def encode(self, x: Tensor) -> VAEEncoderOutput:\n",
    "        pre_z = self.encoder(x)\n",
    "\n",
    "        mean = self.mean_layer(pre_z)\n",
    "        log_variance = self.log_variance_layer(pre_z)\n",
    "\n",
    "        z = self.reparametrization_trick(mean, log_variance)\n",
    "\n",
    "        return VAEEncoderOutput(mean=mean, log_variance=log_variance, latent=z)\n",
    "\n",
    "    def forward(self, x: Tensor) -> VAEOutput:\n",
    "        enc_out = self.encode(x)\n",
    "\n",
    "        output = self.decoder(enc_out.latent)\n",
    "\n",
    "        return VAEOutput(encoder=enc_out, decoder=output)\n",
    "\n",
    "    def reparametrization_trick(self, mean: Tensor, log_variance: Tensor) -> Tensor:\n",
    "        epsilon = rand_normal(zeros(mean.data.shape), ones(log_variance.data.shape)).to(\n",
    "            mean.device\n",
    "        )\n",
    "\n",
    "        return mean + exponential(0.5 * log_variance) * epsilon\n",
    "\n",
    "\n",
    "vae_model = VAE(input_dimension=784, latent_dimension=8)\n",
    "vae_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1972e2bf",
   "metadata": {},
   "source": [
    "## Training the VAE on MNIST\n",
    "\n",
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cebc3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "from deep_learner.datasets import mnist\n",
    "\n",
    "\n",
    "def preprocess(x) -> NDArray:\n",
    "    return (x - np.mean(x, axis=0)) / (np.std(x, axis=0) + 1e-6)\n",
    "\n",
    "\n",
    "def load_mnist() -> tuple[NDArray, NDArray]:\n",
    "    train_X, _, test_X, _ = mnist()\n",
    "    train_X = preprocess(train_X.reshape(-1, 784))\n",
    "    test_X = preprocess(test_X.reshape(-1, 784))\n",
    "\n",
    "    return train_X, test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f609a2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "() ()\n",
      "Initial losses: train_loss=0.38980343706296144, test_loss=0.6279515733548986\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from deep_learner import Device, Tensor\n",
    "from deep_learner.utils import batch\n",
    "from deep_learner.nn.optimizer import SGD\n",
    "\n",
    "\n",
    "def compute_losses() -> tuple[float, float]:\n",
    "    train_output = vae_model(Tensor(train_X).to(DEVICE))\n",
    "    test_output = vae_model(Tensor(test_X).to(DEVICE))\n",
    "\n",
    "    train_loss = vae_model.compute_loss(\n",
    "        x=train_output.decoder,\n",
    "        y=Tensor(train_X).to(DEVICE),\n",
    "        mean_=train_output.encoder.mean,\n",
    "        log_variance=train_output.encoder.log_variance,\n",
    "    )\n",
    "    test_loss = vae_model.compute_loss(\n",
    "        x=test_output.decoder,\n",
    "        y=Tensor(test_X).to(DEVICE),\n",
    "        mean_=test_output.encoder.mean,\n",
    "        log_variance=test_output.encoder.log_variance,\n",
    "    )\n",
    "\n",
    "    return float(train_loss.data), float(test_loss.data)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "DEVICE = Device.CUDA\n",
    "EPOCHS = 10\n",
    "\n",
    "train_X, test_X = load_mnist()\n",
    "\n",
    "optimizer = SGD(vae_model, learning_rate=5e-4)\n",
    "num_batches: int = train_X.shape[0] // BATCH_SIZE\n",
    "\n",
    "vae_model.to(DEVICE)\n",
    "\n",
    "train_loss, test_loss = compute_losses()\n",
    "\n",
    "print(f\"Initial losses: {train_loss=}, {test_loss=}\")\n",
    "\n",
    "train_start = time.perf_counter()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    vae_model.train()\n",
    "\n",
    "    train_accuracy = Tensor(0)\n",
    "\n",
    "    for batch_X in batch(train_X, batch_size=BATCH_SIZE):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_X = Tensor(batch_X).to(DEVICE)\n",
    "\n",
    "        batch_output = vae_model(batch_X)\n",
    "\n",
    "        loss = vae_model.compute_loss(\n",
    "            x=batch_output.decoder,\n",
    "            y=batch_X,\n",
    "            mean_=batch_output.encoder.mean,\n",
    "            log_variance=batch_output.encoder.log_variance,\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    vae_model.eval()\n",
    "    test_y_hat = vae_model(Tensor(test_X).to(DEVICE))\n",
    "\n",
    "    cumulative_time = time.perf_counter() - train_start\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch + 1:>2}: \"\n",
    "        f\"loss={total_loss.data / num_batches:.4f}, \"\n",
    "        f\"train accuracy={train_accuracy.data / num_batches:2.2%}, \"\n",
    "        f\"test accuracy={test_accuracy:2.2%} \"\n",
    "        f\"[{cumulative_time // 60:0>2.0f}:{cumulative_time % 60:0>2.0f}]\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a8e02e",
   "metadata": {},
   "source": [
    "## Appendix A - Proving the positivity of the Kullback-Leibler divergence\n",
    "\n",
    "Given two probability distributions $p$ and $q$, we write :\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        D_{KL}(q || p) & = \\int_x q(x) \\log \\left( \\frac{q(x)}{p(x)} \\right) dx \\\\\n",
    "        & = - \\int_x q(x) \\log \\left( \\frac{p(x)}{q(x)} \\right) dx \\\\\n",
    "        & = \\mathbb{E}_{x \\sim q} \\left[ - \\log \\left( \\frac{p(x)}{q(x)} \\right) \\right]\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "Following Jensen's inequality [[8]](#sources), given that the $-\\log$ function is convex, we can write :\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        D_{KL} (q || p) & \\ge -\\log \\left( \\mathbb{E}_{x \\sim q} \\left[ \\frac{p(x)}{q(x)} \\right] \\right) \\\\\n",
    "        & \\ge -\\log \\left( \\int_x q(x) \\frac{p(x)}{q(x)} dx \\right) \\\\\n",
    "        & \\ge -\\log \\left( \\int_x p(x) dx \\right) \\\\\n",
    "        & \\ge -\\log(1) = 0\n",
    "    \\end{aligned}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d495f4",
   "metadata": {},
   "source": [
    "## Appendix B - KL-divergence between two normal distributions\n",
    "\n",
    "Let $p_a \\sim \\mathcal{N}(\\mu_a, \\sigma_a^2)$ and $p_b\\sim \\mathcal{N}(\\mu_b, \\sigma_b^2)$ be two univariate normal distributions. Let's calculate the KL-divergence between these two distributions :\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\begin{aligned}\n",
    "        D_{KL}(p_a || p_b) & = \\int_x \\frac{1}{\\sqrt{2\\pi\\sigma_a^2}} e^{-\\frac{1}{2} \\left( \\frac{x - \\mu_a}{\\sigma_a} \\right)^2} \\log \\left( \\frac{\\frac{1}{\\sqrt{2\\pi\\sigma_a^2}} e^{-\\frac{1}{2} \\left( \\frac{x - \\mu_a}{\\sigma_a} \\right)^2}}{\\frac{1}{\\sqrt{2\\pi\\sigma_b^2}} e^{-\\frac{1}{2} \\left( \\frac{x - \\mu_b}{\\sigma_b} \\right)^2}} \\right) dx \\\\\n",
    "        & = \\int_x \\frac{1}{\\sqrt{2\\pi\\sigma_a^2}} e^{-\\frac{1}{2} \\left( \\frac{x - \\mu_a}{\\sigma_a} \\right)^2} \\log \\left( \\frac{\\sigma_b}{\\sigma_a} \\times e^{\\frac{1}{2} \\left[ \\left( \\frac{x - \\mu_b}{\\sigma_b} \\right)^2 - \\left( \\frac{x - \\mu_a}{\\sigma_a} \\right)^2 \\right]} \\right) dx \\\\\n",
    "        & = \\int_x \\frac{1}{\\sqrt{2\\pi\\sigma_a^2}} e^{-\\frac{1}{2} \\left( \\frac{x - \\mu_a}{\\sigma_a} \\right)^2} \\log \\left( \\frac{\\sigma_b}{\\sigma_a} \\right) dx + \\int_x \\frac{1}{\\sqrt{2\\pi\\sigma_a^2}} e^{-\\frac{1}{2} \\left( \\frac{x - \\mu_a}{\\sigma_a} \\right)^2} \\log \\left( e^{\\frac{1}{2} \\left[ \\left( \\frac{x - \\mu_b}{\\sigma_b} \\right)^2 - \\left( \\frac{x - \\mu_a}{\\sigma_a} \\right)^2 \\right]} \\right) dx \\\\\n",
    "        & = \\log \\left( \\frac{\\sigma_b}{\\sigma_a} \\right) \\mathbb{E}_{x \\sim p_a}v[p_a(x)] + \\int_x \\frac{1}{\\sqrt{2\\pi\\sigma_a^2}} e^{-\\frac{1}{2} \\left( \\frac{x - \\mu_a}{\\sigma_a} \\right)^2} \\times \\frac{1}{2} \\left[ \\left( \\frac{x - \\mu_b}{\\sigma_b} \\right)^2 - \\left( \\frac{x - \\mu_a}{\\sigma_a} \\right)^2 \\right] dx \\\\\n",
    "        & = \\log \\left( \\frac{\\sigma_b}{\\sigma_a} \\right) \\times \\mu_a + \\frac{1}{2} \\mathbb{E}_{x \\sim p_a} \\left[ \\left( \\frac{x - \\mu_b}{\\sigma_b} \\right)^2 - \\left( \\frac{x - \\mu_a}{\\sigma_a} \\right)^2 \\right] \\\\\n",
    "        & = \\log \\left( \\frac{\\sigma_b}{\\sigma_a} \\right) \\times \\mu_a + \\frac{1}{2\\sigma_b^2} \\mathbb{E}_{x \\sim p_a} [(x - \\mu_b)^2] - \\frac{1}{2\\sigma_a^2}\\mathbb{E}_{x \\sim p_a} [(x - \\mu_a)^2] \\\\\n",
    "        & = \\log \\left( \\frac{\\sigma_b}{\\sigma_a} \\right) \\times \\mu_a - \\frac{1}{2} + \\frac{1}{2\\sigma_b^2} \\mathbb{E}_{x \\sim p_a} [(x - \\mu_a + \\mu_a - \\mu_b)^2] \\\\\n",
    "        & = \\log \\left( \\frac{\\sigma_b}{\\sigma_a} \\right) \\times \\mu_a - \\frac{1}{2} + \\frac{1}{2\\sigma_b^2} \\mathbb{E}_{x \\sim p_a} [(x - \\mu_a)^2 + 2 (x - \\mu_a)(\\mu_a - \\mu_b) + (\\mu_a - \\mu_b)^2] \\\\\n",
    "        & = \\log \\left( \\frac{\\sigma_b}{\\sigma_a} \\right) \\times \\mu_a - \\frac{1}{2} + \\frac{1}{2\\sigma_b^2} \\mathbb{E}_{x \\sim p_a} [(x - \\mu_a)^2] + \\frac{(\\mu_a - \\mu_b)}{\\sigma_b^2} \\mathbb{E}_{x \\sim p_a} [x - \\mu_a] + \\frac{1}{2\\sigma_b^2} \\mathbb{E}_{x \\sim p_a} [(\\mu_a - \\mu_b)^2] \\\\\n",
    "        & = \\log \\left( \\frac{\\sigma_b}{\\sigma_a} \\right) \\times \\mu_a - \\frac{1}{2} + \\frac{\\sigma_a^2}{2\\sigma_b^2} + \\frac{(\\mu_a - \\mu_b)}{\\sigma_b^2} (\\mathbb{E}_{x \\sim p_a} [x] - \\mu_a) + \\frac{(\\mu_a - \\mu_b)^2}{2\\sigma_b^2} \\\\\n",
    "        & = \\log \\left( \\frac{\\sigma_b}{\\sigma_a} \\right) \\times \\mu_a - \\frac{1}{2} + \\frac{\\sigma_a^2}{2\\sigma_b^2} + \\frac{(\\mu_a - \\mu_b)^2}{2\\sigma_b^2} \\\\\n",
    "        & = \\frac{\\sigma_a^2 + (\\mu_a - \\mu_b)^2}{2\\sigma_b^2} + \\log \\left( \\frac{\\sigma_b}{\\sigma_a} \\right) - \\frac{1}{2}\n",
    "    \\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "Now, in the special case where $p_b \\sim \\mathcal{N}(0, 1)$, i.e $\\mu_b = 0$ and $\\sigma_b^2 = 1$, we find that \n",
    "\n",
    "\\begin{align*}\n",
    "    D_{KL}(p_a || p_b) = \\frac{1}{2} \\left[ \\sigma_a^2 + \\mu_a^2 - \\log(\\sigma_a^2) - 1 \\right]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39623d54",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "* [1] [Auto-Encoding Variational Bayes](https://arxiv.org/pdf/1312.6114)\n",
    "* [2] [Understanding Variational Autoencoders (VAEs)](https://youtu.be/HBYQvKlaE0A?si=ttZuI24itPkk6Efz)\n",
    "* [3] [Variational Baesyan Methods](https://en.wikipedia.org/wiki/Variational_Bayesian_methods)\n",
    "* [4] [Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)\n",
    "* [5] [An Introduction to Variational Autoencoders](https://arxiv.org/pdf/1906.02691)\n",
    "* [6] [Variational Autoencoder - Model, ELBO, loss function and maths explained easily!](https://youtu.be/iwEzwTTalbg?si=KNeqkd_TViccLjK4)\n",
    "* [7] [Mathing the Variational Autoencoder: Deriving the ELBO Loss](https://youtu.be/jJZadDULoH4?si=wMo2xyQ6vzAt3lIq)\n",
    "* [8] [Jensen's inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
